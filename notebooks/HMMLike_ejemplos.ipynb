{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos del HMM Like\n",
    "\n",
    "### Nombre\n",
    "\n",
    "* baseline_HMMLike_zero_50_0_0_k_10.crfsuite\n",
    "\n",
    "### Parametros\n",
    "\n",
    "* l1 = 0\n",
    "* l2 = 0\n",
    "* Precisión = 0.8762\n",
    "* Max_Iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycrfsuite\n",
    "import sys  \n",
    "import random\n",
    "sys.path.insert(0, '../')\n",
    "from utils import get_corpus, WordsToLetter, extractLabels, extractTokens\n",
    "from corpus_utils import oto_glosser\n",
    "model = \"baseline_HMMLike_zero_50_0_0_k_10.crfsuite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/baseline/tsu_baseline_HMMLike_zero_50_0_0_k_10.crfsuite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f86484eb050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(\"../models/baseline\", \"tsu_\" + model)\n",
    "print(model_path)\n",
    "tagger = pycrfsuite.Tagger()\n",
    "# Cargando modelos preentrenados\n",
    "tagger.open(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo corpus para pruebas\n",
    "corpus = get_corpus('corpus_otomi_mod', '../corpora/') + get_corpus('corpus_hard', '../corpora/')\n",
    "letter_corpus = WordsToLetter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones auxiliares\n",
    "def obtener_palabras(frases):\n",
    "    palabras = []\n",
    "    for frase in frases:\n",
    "        chunks = [palabra[0] for palabra in frase[:-1]]\n",
    "        palabras.append(\"\".join(chunks))\n",
    "    return palabras\n",
    "\n",
    "def reporte(prediction_tags, real_tags, example):\n",
    "    print(\"Letra | Predicción | Real | Es correcto?\")\n",
    "    for prediction, real, letter in zip(prediction_tags, real_tags, extractTokens(example)):\n",
    "        print(f\"{letter} | {prediction} | {real} | {True if prediction == real else False}\")\n",
    "        \n",
    "def accuracy_score(y_test, y_pred):\n",
    "    right, total = 0, 0\n",
    "    for real, prediction in zip(y_test, y_pred):\n",
    "        if real == prediction:\n",
    "            right += 1\n",
    "    return right / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_functions_maker(sent):\n",
    "    ''' Reglas que configuran las feature functions para entrenamiento\n",
    "\n",
    "    :param sent: Data as `[[[[[letter, POS, BIO-label],...],words],sents]]`\n",
    "    :type: list\n",
    "    :return: list of words with characters as features list:\n",
    "        [[[[[letterfeatures],POS,BIO-label],letters],words]]\n",
    "    :rtype: list\n",
    "    '''\n",
    "\n",
    "    featurelist = []\n",
    "    senlen = len(sent)\n",
    "    # each word in a sentence\n",
    "    for i in range(senlen):\n",
    "        word = sent[i]\n",
    "        wordlen = len(word)\n",
    "        lettersequence = ''\n",
    "        # each letter in a word\n",
    "        for j in range(wordlen):\n",
    "            letter = word[j][0]\n",
    "            # gathering previous letters\n",
    "            lettersequence += letter\n",
    "            # ignore digits\n",
    "            if not letter.isdigit():\n",
    "                features = [\n",
    "                    'bias',\n",
    "                    'letterLowercase=' + letter.lower(),\n",
    "                ]\n",
    "                \n",
    "                if j >= 1:\n",
    "                    features.append('prevletter=' + lettersequence[j-1:j].lower() + '>')\n",
    "             \n",
    "            # Add encoding for pysrfsuite\n",
    "            featurelist.append([f.encode('utf-8') for f in features])\n",
    "    return featurelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Peores ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peores = []\n",
    "prediction_tags = []\n",
    "for i, example in enumerate(letter_corpus):\n",
    "    features = feature_functions_maker(example)\n",
    "    try:\n",
    "        prediction_tags = tagger.tag(features)\n",
    "    except UnicodeDecodeError as e:\n",
    "        # TODO: Revisar que hacer\n",
    "        print(f\"Error generando {e.object} > {e.object.decode}\")\n",
    "        continue     \n",
    "    real_tags = extractLabels(example, 1)\n",
    "    accuracy = accuracy_score(real_tags, prediction_tags)\n",
    "    if accuracy <= 0.8:\n",
    "        peores.append((i, accuracy))\n",
    "peores = sorted(peores, key=lambda t: t[1])\n",
    "print(len(peores), len(letter_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = peores[:5]\n",
    "for i, data in enumerate(top):\n",
    "    print(\"-\"*50)\n",
    "    index = data[0]\n",
    "    palabras = obtener_palabras(corpus[index])\n",
    "    example = letter_corpus[index]\n",
    "    features = feature_functions_maker(example)\n",
    "    try:\n",
    "        prediction_tags = tagger.tag(features)\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(e.object)\n",
    "        continue\n",
    "    real_tags = extractLabels(example, 1)\n",
    "    print(f\"Ejemplo {i+1} de 5 | Frase: \\\"{' '.join(palabras)}\\\" | Precisión: {data[1]}\")\n",
    "    reporte(prediction_tags, real_tags, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejores ejemplos\n",
    "\n",
    "Como de esos hay muchos tomaremos 3 al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "while flag != 3:\n",
    "    index = random.randint(0, len(letter_corpus))\n",
    "    palabras = obtener_palabras(corpus[index])\n",
    "    example = letter_corpus[index]\n",
    "    features = feature_functions_maker(example)\n",
    "    prediction_tags = tagger.tag(features)\n",
    "    real_tags = extractLabels(example, 1)\n",
    "    accuracy = accuracy_score(real_tags, prediction_tags)\n",
    "    if accuracy >= 0.99:\n",
    "        print(f\"Ejemplo {flag + 1} de 3 | Frase: \\\"{' '.join(palabras)}\\\" | Precisión: {accuracy}\")\n",
    "        reporte(prediction_tags, real_tags, example)\n",
    "        flag += 1\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([\"bi'μngí\", 'yι', 'mbμhí', 'nge', 'hín', 'dímáné', 'gwaporá', 'nge', 'dímádáhní'], [[['bi', '3.cpl'], [\"'μn\", 'stem'], ['gí', '1.obj'], 'v'], [['yι', 'det.pl'], 'det'], [['mbμhí', 'stem'], 'obl'], [['nge', 'stem'], 'cnj'], [['hín', 'stem'], 'neg'], [['dí', '1.icp'], ['má', 'ctrf'], ['né', 'stem'], 'v'], [['gwa', '1.icp.irr'], ['porá', 'stem'], 'v'], [['nge', 'stem'], 'cnj'], [['dí', '1.icp'], ['má', 'ctrf'], ['dáhní', 'stem'], 'v']], 1, 1.0), (['komo', 'beinte', 'o', 'treinta', 'pero', 'despwés', 'xo', 'binkhμtsí'], [[['komo', 'stem'], 'obl'], [['beinte', 'stem'], 'obl'], [['o', 'stem'], 'obl'], [['treinta', 'stem'], 'obl'], [['pero', 'stem'], 'obl'], [['despwés', 'stem'], 'obl'], [['xo', 'stem'], 'cnj'], [['bi', '3.cpl'], ['nkhμtsí', 'stem'], 'v']], 56, 1.0), (['xo', 'mitahá', 'rι', 'ntsoyι', 'mitá', 'sínko', 'pesos', 'rι', 'béhñι', 'dos', 'sinkwenta'], [[['xo', 'stem'], 'cnj'], [['m', 'psd'], ['i', '3.icp'], ['tahá', 'stem'], 'v'], [['rι', 'det'], 'det'], [['ntsoyι', 'stem'], 'obl'], [['m', 'psd'], ['i', '3.icp'], ['tá', 'stem'], 'v'], [['sínko', 'stem'], 'obl'], [['pesos', 'stem'], 'obl'], [['rι', 'det'], 'det'], [['béhñι', 'stem'], 'obl'], [['dos', 'stem'], 'obl'], [['sinkwenta', 'stem'], 'obl']], 62, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Obteniendo las frases mas largas y mejor etiquetadas\n",
    "min_len = 30\n",
    "phrases = []\n",
    "for i, example in enumerate(letter_corpus):\n",
    "    features = feature_functions_maker(example)\n",
    "    palabras = obtener_palabras(corpus[i])\n",
    "    prediction_tags = tagger.tag(features)\n",
    "    real_tags = extractLabels(example, 1)\n",
    "    accuracy = accuracy_score(real_tags, prediction_tags)\n",
    "    if accuracy >= 0.99:\n",
    "        if len(\"\".join(palabras)) >= min_len:\n",
    "            predicted_gloss = oto_glosser(palabras, prediction_tags, corpus[i])\n",
    "            phrases.append((palabras, predicted_gloss, i, accuracy))\n",
    "print(phrases[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"mejores-frases-HMMLike.csv\", \"w\") as csvfile:\n",
    "    header = [\"frase\", \"glosa_predicha\", \"glosa_real\", \"eq?\",'accuracy-score']\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for phrase in phrases:\n",
    "        writer.writerow([\" \".join(phrase[0]), str(phrase[1]),\n",
    "                         corpus[phrase[2]], phrase[1] == corpus[phrase[2]],\n",
    "                         phrase[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['gótú', 'riáste', 'ripetrólio', 'rixábo', 'rinιni', 'i', 'gothó', 'tantsμhní'], [[['gótú', 'stem'], 'v'], [['riáste', 'stem'], 'n'], [['ri', '2.pss'], ['petrólio', 'stem'], 'n'], [['rixá', 'stem'], ['bo', '3.cpl'], 'n'], [['ri', 'stem'], ['nι', 'dem'], ['ni', 'stem'], 'n'], [['i', '3.icp'], 'cnj'], [['gothó', 'stem'], 'obl'], [['tantsμhní', 'stem'], 'unkwn']], 416, 0.44680851063829785)]\n"
     ]
    }
   ],
   "source": [
    "# Obteniendo las frases mas largas y mejor etiquetadas\n",
    "min_len = 30\n",
    "phrases = []\n",
    "for i, example in enumerate(letter_corpus):\n",
    "    features = feature_functions_maker(example)\n",
    "    palabras = obtener_palabras(corpus[i])\n",
    "    prediction_tags = tagger.tag(features)\n",
    "    real_tags = extractLabels(example, 1)\n",
    "    accuracy = accuracy_score(real_tags, prediction_tags)\n",
    "    if accuracy <= 0.50:\n",
    "        if len(\"\".join(palabras)) >= min_len:\n",
    "            predicted_gloss = oto_glosser(palabras, prediction_tags, corpus[i])\n",
    "            phrases.append((palabras, predicted_gloss, i, accuracy))\n",
    "print(phrases[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"peores-frases-HMMLike.csv\", \"w\") as csvfile:\n",
    "    header = [\"frase\", \"glosa_predicha\", \"glosa_real\", \"eq?\",'accuracy-score']\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for phrase in phrases:\n",
    "        writer.writerow([\" \".join(phrase[0]), str(phrase[1]),\n",
    "                         corpus[phrase[2]], phrase[1] == corpus[phrase[2]],\n",
    "                         phrase[3]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesis Kernel",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
