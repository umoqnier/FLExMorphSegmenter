{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Prediction of Lezgi Morpheme Breaks\n",
    "\n",
    "This program does supervised morphological analysis and glossing of affixes. It is intended to quickly increase the amount of accessible data from low resource, and often endangered, languages. This classifier can be used on any language but it expects 2000-3000 words of cleanly annotated data. \n",
    "\n",
    "This example is designed for Lezgi [lez], a Nakh-Daghestanian language spoken in Russia and Azerbaijan. Lezgi is an agglutinating language that is overwhelmingly suffixing. The training and test data came from a collection of 21 transcribed oral narratives spoken in the Qusar dialect of northwest Azerbaijan. Nine texts with about 2,500 words were used for training data after having been cleanly annotated with morpheme breaks and part of speech. All but three of affixes were glossed. Many of the stems are not glossed. The FlexText XML export labels each morpheme as stem, suffix, or prefix. \n",
    "\n",
    "This program is considered successful if it reaches 80% accuracy. This goal comes from the Pareto Principle - the idea that 20% of one's effort produces 80% of one's results, and vice versa. This program should accurately complete 80% of the annotations, leaving the most interesting and informative 20% for the human linguist to complete.This project was inspired by an ongoing fieldwork project. A native Lezgi speaker who has no background in linguistics has been annotating the collection of texts. She has quickly learned basic morphology and gained FLEx skills. However, simultaneously learning and doing basic linguistic analysis produces inaccurate and inconsistent annotations. It is also time-consuming. Many of the mistakes are due to the repetitive nature of the work. Not every part of speech has inflectional morphology. The annotator is most likely to skip over essential words with simple morphology, such as ergative case-marked arguments, and concentrate on morphologicaly complex words. \n",
    "\n",
    "Once the training is complete, the program should predict morpheme breaks and affix glosses for any text that has been labeled with parts of speech. Identifying parts of speech is required because this seems a reasonable task for a non-linguist native speaker. The data used in this example does include two distinctions in Lezgi that might be difficult without linguistic training. Participles are distinguished from verbs, but Lezgi participles end in a unique letter. Demonstrative pronouns are distinguished from pronouns. This distinction was used primarily because it was already consistently annotated in the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Preprocessing Data\n",
    " \n",
    "This process assumes that 1) the data has been analyzed in FLEx and exported as a FlexText, then saved with an .xml file extension, 2) words have been annotated in FLEx for part of speech, (for this example - verb, participle, adjective, adverb, noun/proper noun, particle, (personal) pronoun, demonstrative, and postposition), 3) morpheme breaks are consistent, and 4) all affixes, but not stems, are glossed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API for parsing XML docs\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XMLtoWords(filename):\n",
    "    '''Takes FLExText text as .xml. Returns data as list: [[[[[[morpheme, gloss], pos],...],words],sents]].\n",
    "    Ignores punctuation. Morph_types can be: stem, suffix, prefix, or phrase when lexical item is made up of two words.'''\n",
    "    \n",
    "    datalists = []\n",
    "\n",
    "    #open XML doc using xml parser\n",
    "    root = ET.parse(filename).getroot()\n",
    "\n",
    "    for text in root:\n",
    "        for paragraphs in text:\n",
    "            #Only get paragraphs, ignore metadata.\n",
    "            if paragraphs.tag == 'paragraphs':\n",
    "                for paragraph in paragraphs:\n",
    "                    #jump straight into items under phrases\n",
    "                    for phrase in paragraph[0]:\n",
    "                        sent = []\n",
    "                        #ignore first item which is the sentence number\n",
    "                        for word in phrase[1]:\n",
    "                            #ignore punctuation tags which have no attributes\n",
    "                            if word.attrib:\n",
    "                                lexeme = []\n",
    "                                for node in word:\n",
    "                                    if node.tag == 'morphemes':\n",
    "                                        for morph in node:\n",
    "                                            morpheme = []\n",
    "                                            #note morph type \n",
    "                                            morph_type = morph.get('type')\n",
    "                                            #Treat MWEs or unlabled morphemes as stems.\n",
    "                                            if morph_type == None or morph_type == 'phrase':\n",
    "                                                morph_type = 'stem'                                            \n",
    "                                            for item in morph:\n",
    "                                                #get morpheme token\n",
    "                                                if item.get('type') == 'txt':\n",
    "                                                    form = item.text\n",
    "                                                    #get rid of hyphens demarcating affixes\n",
    "                                                    if morph_type == 'suffix':\n",
    "                                                        form = form[1:]\n",
    "                                                    if morph_type == 'prefix':\n",
    "                                                        form = form[:-1]\n",
    "                                                    morpheme.append(form)\n",
    "                                                #get affix glosses\n",
    "                                                if item.get('type') == 'gls' and morph_type != 'stem':\n",
    "                                                    morpheme.append(item.text)\n",
    "                                            #get stem \"gloss\" = 'stem'\n",
    "                                            if morph_type == 'stem':\n",
    "                                                morpheme.append(morph_type)\n",
    "                                            lexeme.append(morpheme)\n",
    "                                    #get word's POS\n",
    "                                    if node.get('type') == 'pos':\n",
    "                                        lexeme.append(node.text)\n",
    "                                sent.append(lexeme)\n",
    "                        datalists.append(sent)\n",
    "    return datalists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsToLetter(wordlists):\n",
    "    '''Takes data from XMLtoWords: [[[[[[morpheme, gloss], pos],...],words],sents]]. \n",
    "    Returns [[[[[letter, POS, BIO-label],...],words],sents]]'''\n",
    "\n",
    "    letterlists = []\n",
    "    \n",
    "    for phrase in wordlists:\n",
    "        sent = []\n",
    "        for lexeme in phrase:\n",
    "            word = []\n",
    "            #Skip POS label\n",
    "            for morpheme in lexeme[:-1]:\n",
    "                #use gloss as BIO label\n",
    "                label = morpheme[1]\n",
    "                #Break morphemes into letters\n",
    "                for i in range(len(morpheme[0])):\n",
    "                    letter = [morpheme[0][i]]\n",
    "                    #add POS label to each letter\n",
    "                    letter.append(lexeme[-1])\n",
    "                    #add BIO label\n",
    "                    if i == 0:\n",
    "                        letter.append('B-' + label)\n",
    "                    else:\n",
    "                        letter.append('I-' + label)\n",
    "                        #letter.append('I')\n",
    "                    word.append(letter)\n",
    "            sent.append(word)\n",
    "        letterlists.append(sent)\n",
    "    \n",
    "    return letterlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call below takes the data from the FLExText XML export. The data is read from the XML file and broken down by morphemes. Then it is broken down by letter. Each letter is associated with the word's part of speech tag and a BIO label. The BIO label for stems is \"stem\". The label for affixes is their gloss. \"B\" denotes the initial letter of a morpheme. I marks non-initial letters.\n",
    "\n",
    "With a corpus of a little less than 2,500 words, I originally tried a 90/10 split. The accuracy results ranged from 92% to 97% but the test data was seeing a dozen or less labels. An 80/20 random split ranges less than 2% in accuracy, but still averages about 94%. However, the number of labels the test data encounters is nearly doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%debug is a cell magic, but the cell body is empty. Did you mean the line magic %debug (single %)?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  64 Tests:  16\n"
     ]
    }
   ],
   "source": [
    "#Randomize and split the data\n",
    "traindata,testdata = train_test_split(WordsToLetter(XMLtoWords(\"FLExTxtExport2.xml\")),test_size=0.2)\n",
    "\n",
    "print(\"Train: \", len(traindata), \"Tests: \", len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['в', 'pro', 'B-stem'], ['и', 'pro', 'I-stem'], ['ч', 'pro', 'I-stem']],\n",
       " [['г', 'n', 'B-stem'],\n",
       "  ['а', 'n', 'I-stem'],\n",
       "  ['д', 'n', 'I-stem'],\n",
       "  ['а', 'n', 'I-stem'],\n",
       "  ['н', 'n', 'B-FOC'],\n",
       "  ['и', 'n', 'I-FOC']],\n",
       " [['к', 'v', 'B-stem'],\n",
       "  ['ъ', 'v', 'I-stem'],\n",
       "  ['у', 'v', 'I-stem'],\n",
       "  ['н', 'v', 'B-AOR'],\n",
       "  ['а', 'v', 'I-AOR']],\n",
       " [['а', 'v', 'B-stem'],\n",
       "  ['х', 'v', 'I-stem'],\n",
       "  ['ъ', 'v', 'I-stem'],\n",
       "  ['а', 'v', 'I-stem'],\n",
       "  ['х', 'v', 'I-stem'],\n",
       "  ['н', 'v', 'B-AOR'],\n",
       "  ['а', 'v', 'I-AOR']],\n",
       " [['т', 'n', 'B-stem'],\n",
       "  ['а', 'n', 'I-stem'],\n",
       "  ['к', 'n', 'I-stem'],\n",
       "  ['с', 'n', 'I-stem'],\n",
       "  ['и', 'n', 'I-stem'],\n",
       "  ['д', 'n', 'B-OBL'],\n",
       "  ['и', 'n', 'I-OBL'],\n",
       "  ['з', 'n', 'B-DAT']],\n",
       " [['х', 'v', 'B-stem'],\n",
       "  ['ъ', 'v', 'I-stem'],\n",
       "  ['ф', 'v', 'I-stem'],\n",
       "  ['е', 'v', 'I-stem'],\n",
       "  ['н', 'v', 'B-AOR'],\n",
       "  ['а', 'v', 'I-AOR']],\n",
       " [['Б', 'nprop', 'B-stem'],\n",
       "  ['и', 'nprop', 'I-stem'],\n",
       "  ['л', 'nprop', 'I-stem'],\n",
       "  ['е', 'nprop', 'I-stem'],\n",
       "  ['с', 'nprop', 'I-stem'],\n",
       "  ['у', 'nprop', 'I-stem'],\n",
       "  ['в', 'nprop', 'I-stem'],\n",
       "  ['а', 'nprop', 'I-stem'],\n",
       "  ['р', 'nprop', 'I-stem'],\n",
       "  ['д', 'nprop', 'B-OBL'],\n",
       "  ['и', 'nprop', 'I-OBL'],\n",
       "  ['з', 'nprop', 'B-DAT']],\n",
       " [['р', 'n', 'B-stem'],\n",
       "  ['е', 'n', 'I-stem'],\n",
       "  ['к', 'n', 'I-stem'],\n",
       "  ['ъ', 'n', 'I-stem'],\n",
       "  ['е', 'n', 'B-INESS']]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRFSuite \n",
    "### Define Features\n",
    "\n",
    "It is assumed that a \"phrase\" in FLEx is equivalent to a complete sentence. In reality, some \"phrases\" contain more than one sentence, some contain only a sentence fragment. This means that the word position in the sentence is often inaccurate, but it was retained to take into account Lezgi's strong tendency for verb-final word order. Affixes are rarely more than 3 letters long, so features include the previous and next 1-4 letters. This ensures that the program is viewing at least one letter in the previous/next morpheme. More often it is viewing the whole previous/next 1-2 morphemes. \n",
    "\n",
    "Since Lezgi is primarily suffixing, the position of a letter in a word is counted from the end of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(sent):\n",
    "    '''Takes data as [[[[[letter, POS, BIO-label],...],words],sents]].\n",
    "    Returns list of words with characters as features list: [[[[[letterfeatures],POS,BIO-label],letters],words]]'''\n",
    "    \n",
    "    featurelist = []\n",
    "    senlen = len(sent)\n",
    "    \n",
    "    #each word in a sentence\n",
    "    for i in range(senlen):\n",
    "        word = sent[i]\n",
    "        wordlen = len(word)\n",
    "        lettersequence = ''\n",
    "        #each letter in a word\n",
    "        for j in range(wordlen):\n",
    "            letter = word[j][0]\n",
    "            #gathering previous letters\n",
    "            lettersequence += letter\n",
    "            #ignore digits             \n",
    "            if not letter.isdigit():\n",
    "                features = [\n",
    "                    'bias',\n",
    "                    'letterLowercase=' + letter.lower(),\n",
    "                    'postag=' + word[j][1],\n",
    "                ] \n",
    "                #position of word in sentence and pos tags sequence\n",
    "                if i > 0:\n",
    "                    features.append('prevpostag=' + sent[i-1][0][1])\n",
    "                    if i != senlen-1:\n",
    "                        features.append('nxtpostag=' + sent[i+1][0][1])\n",
    "                    else:\n",
    "                        features.append('EOS')\n",
    "                else:\n",
    "                    features.append('BOS')\n",
    "                    #Don't get pos tag if sentence is 1 word long\n",
    "                    if i != senlen-1:\n",
    "                        features.append('nxtpostag=' + sent[i+1][0][1])\n",
    "                #position of letter in word\n",
    "                if j == 0:\n",
    "                    features.append('BOW')\n",
    "                elif j == wordlen-1:\n",
    "                    features.append('EOW')\n",
    "                else:\n",
    "                    features.append('letterposition=-%s' % str(wordlen-1-j))\n",
    "                #letter sequences before letter\n",
    "                if j >= 4:\n",
    "                    features.append('prev4letters=' + lettersequence[j-4:j].lower() + '>')\n",
    "                if j >= 3:\n",
    "                    features.append('prev3letters=' + lettersequence[j-3:j].lower() + '>')\n",
    "                if j >= 2:\n",
    "                    features.append('prev2letters=' + lettersequence[j-2:j].lower() + '>')\n",
    "                if j >= 1:\n",
    "                    features.append('prevletter=' + lettersequence[j-1:j].lower() + '>')\n",
    "                #letter sequences after letter\n",
    "                if j <= wordlen-2:\n",
    "                    nxtlets = word[j+1][0]\n",
    "                    features.append('nxtletter=<' + nxtlets.lower())\n",
    "                    #print('\\nnextletter:', nxtlet)\n",
    "                if j <= wordlen-3:\n",
    "                    nxtlets += word[j+2][0]\n",
    "                    features.append('nxt2letters=<' + nxtlets.lower())\n",
    "                    #print('next2let:', nxt2let)\n",
    "                if j <= wordlen-4:\n",
    "                    nxtlets += word[j+3][0]\n",
    "                    features.append('nxt3letters=<' + nxtlets.lower())\n",
    "                if j <= wordlen-5:\n",
    "                    nxtlets += word[j+4][0]\n",
    "                    features.append('nxt4letters=<' + nxtlets.lower())\n",
    "                \n",
    "            featurelist.append(features)\n",
    "    \n",
    "    return featurelist\n",
    "\n",
    "def extractLabels(sent):\n",
    "    labels = []\n",
    "    for word in sent:\n",
    "        for letter in word:\n",
    "            labels.append(letter[2])\n",
    "    return labels\n",
    "\n",
    "def extractTokens(sent):\n",
    "    tokens = []\n",
    "    for word in sent:\n",
    "        for letter in word:\n",
    "            tokens.append(letter[0])\n",
    "    return tokens\n",
    "\n",
    "def sent2features(data):\n",
    "    return [extractFeatures(sent) for sent in data]\n",
    "\n",
    "def sent2labels(data):\n",
    "    return [extractLabels(sent) for sent in data]\n",
    "\n",
    "def sent2tokens(data):\n",
    "    return [extractTokens(sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2features(traindata)\n",
    "Y_train = sent2labels(traindata)\n",
    "\n",
    "X_test = sent2features(testdata)\n",
    "Y_test = sent2labels(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bias',\n",
       "  'letterLowercase=в',\n",
       "  'postag=pro',\n",
       "  'BOS',\n",
       "  'nxtpostag=n',\n",
       "  'BOW',\n",
       "  'nxtletter=<и',\n",
       "  'nxt2letters=<ич'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=pro',\n",
       "  'BOS',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-1',\n",
       "  'prevletter=в>',\n",
       "  'nxtletter=<ч'],\n",
       " ['bias',\n",
       "  'letterLowercase=ч',\n",
       "  'postag=pro',\n",
       "  'BOS',\n",
       "  'nxtpostag=n',\n",
       "  'EOW',\n",
       "  'prev2letters=ви>',\n",
       "  'prevletter=и>'],\n",
       " ['bias',\n",
       "  'letterLowercase=г',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'BOW',\n",
       "  'nxtletter=<а',\n",
       "  'nxt2letters=<ад',\n",
       "  'nxt3letters=<ада',\n",
       "  'nxt4letters=<адан'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-4',\n",
       "  'prevletter=г>',\n",
       "  'nxtletter=<д',\n",
       "  'nxt2letters=<да',\n",
       "  'nxt3letters=<дан',\n",
       "  'nxt4letters=<дани'],\n",
       " ['bias',\n",
       "  'letterLowercase=д',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-3',\n",
       "  'prev2letters=га>',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<а',\n",
       "  'nxt2letters=<ан',\n",
       "  'nxt3letters=<ани'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-2',\n",
       "  'prev3letters=гад>',\n",
       "  'prev2letters=ад>',\n",
       "  'prevletter=д>',\n",
       "  'nxtletter=<н',\n",
       "  'nxt2letters=<ни'],\n",
       " ['bias',\n",
       "  'letterLowercase=н',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-1',\n",
       "  'prev4letters=гада>',\n",
       "  'prev3letters=ада>',\n",
       "  'prev2letters=да>',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<и'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=n',\n",
       "  'prevpostag=pro',\n",
       "  'nxtpostag=v',\n",
       "  'EOW',\n",
       "  'prev4letters=адан>',\n",
       "  'prev3letters=дан>',\n",
       "  'prev2letters=ан>',\n",
       "  'prevletter=н>'],\n",
       " ['bias',\n",
       "  'letterLowercase=к',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=v',\n",
       "  'BOW',\n",
       "  'nxtletter=<ъ',\n",
       "  'nxt2letters=<ъу',\n",
       "  'nxt3letters=<ъун',\n",
       "  'nxt4letters=<ъуна'],\n",
       " ['bias',\n",
       "  'letterLowercase=ъ',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-3',\n",
       "  'prevletter=к>',\n",
       "  'nxtletter=<у',\n",
       "  'nxt2letters=<ун',\n",
       "  'nxt3letters=<уна'],\n",
       " ['bias',\n",
       "  'letterLowercase=у',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-2',\n",
       "  'prev2letters=къ>',\n",
       "  'prevletter=ъ>',\n",
       "  'nxtletter=<н',\n",
       "  'nxt2letters=<на'],\n",
       " ['bias',\n",
       "  'letterLowercase=н',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-1',\n",
       "  'prev3letters=къу>',\n",
       "  'prev2letters=ъу>',\n",
       "  'prevletter=у>',\n",
       "  'nxtletter=<а'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=v',\n",
       "  'EOW',\n",
       "  'prev4letters=къун>',\n",
       "  'prev3letters=ъун>',\n",
       "  'prev2letters=ун>',\n",
       "  'prevletter=н>'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'BOW',\n",
       "  'nxtletter=<х',\n",
       "  'nxt2letters=<хъ',\n",
       "  'nxt3letters=<хъа',\n",
       "  'nxt4letters=<хъах'],\n",
       " ['bias',\n",
       "  'letterLowercase=х',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-5',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<ъ',\n",
       "  'nxt2letters=<ъа',\n",
       "  'nxt3letters=<ъах',\n",
       "  'nxt4letters=<ъахн'],\n",
       " ['bias',\n",
       "  'letterLowercase=ъ',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-4',\n",
       "  'prev2letters=ах>',\n",
       "  'prevletter=х>',\n",
       "  'nxtletter=<а',\n",
       "  'nxt2letters=<ах',\n",
       "  'nxt3letters=<ахн',\n",
       "  'nxt4letters=<ахна'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-3',\n",
       "  'prev3letters=ахъ>',\n",
       "  'prev2letters=хъ>',\n",
       "  'prevletter=ъ>',\n",
       "  'nxtletter=<х',\n",
       "  'nxt2letters=<хн',\n",
       "  'nxt3letters=<хна'],\n",
       " ['bias',\n",
       "  'letterLowercase=х',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-2',\n",
       "  'prev4letters=ахъа>',\n",
       "  'prev3letters=хъа>',\n",
       "  'prev2letters=ъа>',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<н',\n",
       "  'nxt2letters=<на'],\n",
       " ['bias',\n",
       "  'letterLowercase=н',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-1',\n",
       "  'prev4letters=хъах>',\n",
       "  'prev3letters=ъах>',\n",
       "  'prev2letters=ах>',\n",
       "  'prevletter=х>',\n",
       "  'nxtletter=<а'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=v',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'EOW',\n",
       "  'prev4letters=ъахн>',\n",
       "  'prev3letters=ахн>',\n",
       "  'prev2letters=хн>',\n",
       "  'prevletter=н>'],\n",
       " ['bias',\n",
       "  'letterLowercase=т',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'BOW',\n",
       "  'nxtletter=<а',\n",
       "  'nxt2letters=<ак',\n",
       "  'nxt3letters=<акс',\n",
       "  'nxt4letters=<акси'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-6',\n",
       "  'prevletter=т>',\n",
       "  'nxtletter=<к',\n",
       "  'nxt2letters=<кс',\n",
       "  'nxt3letters=<кси',\n",
       "  'nxt4letters=<ксид'],\n",
       " ['bias',\n",
       "  'letterLowercase=к',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-5',\n",
       "  'prev2letters=та>',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<с',\n",
       "  'nxt2letters=<си',\n",
       "  'nxt3letters=<сид',\n",
       "  'nxt4letters=<сиди'],\n",
       " ['bias',\n",
       "  'letterLowercase=с',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-4',\n",
       "  'prev3letters=так>',\n",
       "  'prev2letters=ак>',\n",
       "  'prevletter=к>',\n",
       "  'nxtletter=<и',\n",
       "  'nxt2letters=<ид',\n",
       "  'nxt3letters=<иди',\n",
       "  'nxt4letters=<идиз'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-3',\n",
       "  'prev4letters=такс>',\n",
       "  'prev3letters=акс>',\n",
       "  'prev2letters=кс>',\n",
       "  'prevletter=с>',\n",
       "  'nxtletter=<д',\n",
       "  'nxt2letters=<ди',\n",
       "  'nxt3letters=<диз'],\n",
       " ['bias',\n",
       "  'letterLowercase=д',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-2',\n",
       "  'prev4letters=акси>',\n",
       "  'prev3letters=кси>',\n",
       "  'prev2letters=си>',\n",
       "  'prevletter=и>',\n",
       "  'nxtletter=<и',\n",
       "  'nxt2letters=<из'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'letterposition=-1',\n",
       "  'prev4letters=ксид>',\n",
       "  'prev3letters=сид>',\n",
       "  'prev2letters=ид>',\n",
       "  'prevletter=д>',\n",
       "  'nxtletter=<з'],\n",
       " ['bias',\n",
       "  'letterLowercase=з',\n",
       "  'postag=n',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=v',\n",
       "  'EOW',\n",
       "  'prev4letters=сиди>',\n",
       "  'prev3letters=иди>',\n",
       "  'prev2letters=ди>',\n",
       "  'prevletter=и>'],\n",
       " ['bias',\n",
       "  'letterLowercase=х',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'BOW',\n",
       "  'nxtletter=<ъ',\n",
       "  'nxt2letters=<ъф',\n",
       "  'nxt3letters=<ъфе',\n",
       "  'nxt4letters=<ъфен'],\n",
       " ['bias',\n",
       "  'letterLowercase=ъ',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'letterposition=-4',\n",
       "  'prevletter=х>',\n",
       "  'nxtletter=<ф',\n",
       "  'nxt2letters=<фе',\n",
       "  'nxt3letters=<фен',\n",
       "  'nxt4letters=<фена'],\n",
       " ['bias',\n",
       "  'letterLowercase=ф',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'letterposition=-3',\n",
       "  'prev2letters=хъ>',\n",
       "  'prevletter=ъ>',\n",
       "  'nxtletter=<е',\n",
       "  'nxt2letters=<ен',\n",
       "  'nxt3letters=<ена'],\n",
       " ['bias',\n",
       "  'letterLowercase=е',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'letterposition=-2',\n",
       "  'prev3letters=хъф>',\n",
       "  'prev2letters=ъф>',\n",
       "  'prevletter=ф>',\n",
       "  'nxtletter=<н',\n",
       "  'nxt2letters=<на'],\n",
       " ['bias',\n",
       "  'letterLowercase=н',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'letterposition=-1',\n",
       "  'prev4letters=хъфе>',\n",
       "  'prev3letters=ъфе>',\n",
       "  'prev2letters=фе>',\n",
       "  'prevletter=е>',\n",
       "  'nxtletter=<а'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=v',\n",
       "  'prevpostag=n',\n",
       "  'nxtpostag=nprop',\n",
       "  'EOW',\n",
       "  'prev4letters=ъфен>',\n",
       "  'prev3letters=фен>',\n",
       "  'prev2letters=ен>',\n",
       "  'prevletter=н>'],\n",
       " ['bias',\n",
       "  'letterLowercase=б',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'BOW',\n",
       "  'nxtletter=<и',\n",
       "  'nxt2letters=<ил',\n",
       "  'nxt3letters=<иле',\n",
       "  'nxt4letters=<илес'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-10',\n",
       "  'prevletter=б>',\n",
       "  'nxtletter=<л',\n",
       "  'nxt2letters=<ле',\n",
       "  'nxt3letters=<лес',\n",
       "  'nxt4letters=<лесу'],\n",
       " ['bias',\n",
       "  'letterLowercase=л',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-9',\n",
       "  'prev2letters=би>',\n",
       "  'prevletter=и>',\n",
       "  'nxtletter=<е',\n",
       "  'nxt2letters=<ес',\n",
       "  'nxt3letters=<есу',\n",
       "  'nxt4letters=<есув'],\n",
       " ['bias',\n",
       "  'letterLowercase=е',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-8',\n",
       "  'prev3letters=бил>',\n",
       "  'prev2letters=ил>',\n",
       "  'prevletter=л>',\n",
       "  'nxtletter=<с',\n",
       "  'nxt2letters=<су',\n",
       "  'nxt3letters=<сув',\n",
       "  'nxt4letters=<сува'],\n",
       " ['bias',\n",
       "  'letterLowercase=с',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-7',\n",
       "  'prev4letters=биле>',\n",
       "  'prev3letters=иле>',\n",
       "  'prev2letters=ле>',\n",
       "  'prevletter=е>',\n",
       "  'nxtletter=<у',\n",
       "  'nxt2letters=<ув',\n",
       "  'nxt3letters=<ува',\n",
       "  'nxt4letters=<увар'],\n",
       " ['bias',\n",
       "  'letterLowercase=у',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-6',\n",
       "  'prev4letters=илес>',\n",
       "  'prev3letters=лес>',\n",
       "  'prev2letters=ес>',\n",
       "  'prevletter=с>',\n",
       "  'nxtletter=<в',\n",
       "  'nxt2letters=<ва',\n",
       "  'nxt3letters=<вар',\n",
       "  'nxt4letters=<вард'],\n",
       " ['bias',\n",
       "  'letterLowercase=в',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-5',\n",
       "  'prev4letters=лесу>',\n",
       "  'prev3letters=есу>',\n",
       "  'prev2letters=су>',\n",
       "  'prevletter=у>',\n",
       "  'nxtletter=<а',\n",
       "  'nxt2letters=<ар',\n",
       "  'nxt3letters=<ард',\n",
       "  'nxt4letters=<арди'],\n",
       " ['bias',\n",
       "  'letterLowercase=а',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-4',\n",
       "  'prev4letters=есув>',\n",
       "  'prev3letters=сув>',\n",
       "  'prev2letters=ув>',\n",
       "  'prevletter=в>',\n",
       "  'nxtletter=<р',\n",
       "  'nxt2letters=<рд',\n",
       "  'nxt3letters=<рди',\n",
       "  'nxt4letters=<рдиз'],\n",
       " ['bias',\n",
       "  'letterLowercase=р',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-3',\n",
       "  'prev4letters=сува>',\n",
       "  'prev3letters=ува>',\n",
       "  'prev2letters=ва>',\n",
       "  'prevletter=а>',\n",
       "  'nxtletter=<д',\n",
       "  'nxt2letters=<ди',\n",
       "  'nxt3letters=<диз'],\n",
       " ['bias',\n",
       "  'letterLowercase=д',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-2',\n",
       "  'prev4letters=увар>',\n",
       "  'prev3letters=вар>',\n",
       "  'prev2letters=ар>',\n",
       "  'prevletter=р>',\n",
       "  'nxtletter=<и',\n",
       "  'nxt2letters=<из'],\n",
       " ['bias',\n",
       "  'letterLowercase=и',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'letterposition=-1',\n",
       "  'prev4letters=вард>',\n",
       "  'prev3letters=ард>',\n",
       "  'prev2letters=рд>',\n",
       "  'prevletter=д>',\n",
       "  'nxtletter=<з'],\n",
       " ['bias',\n",
       "  'letterLowercase=з',\n",
       "  'postag=nprop',\n",
       "  'prevpostag=v',\n",
       "  'nxtpostag=n',\n",
       "  'EOW',\n",
       "  'prev4letters=арди>',\n",
       "  'prev3letters=рди>',\n",
       "  'prev2letters=ди>',\n",
       "  'prevletter=и>'],\n",
       " ['bias',\n",
       "  'letterLowercase=р',\n",
       "  'postag=n',\n",
       "  'prevpostag=nprop',\n",
       "  'EOS',\n",
       "  'BOW',\n",
       "  'nxtletter=<е',\n",
       "  'nxt2letters=<ек',\n",
       "  'nxt3letters=<екъ',\n",
       "  'nxt4letters=<екъе'],\n",
       " ['bias',\n",
       "  'letterLowercase=е',\n",
       "  'postag=n',\n",
       "  'prevpostag=nprop',\n",
       "  'EOS',\n",
       "  'letterposition=-3',\n",
       "  'prevletter=р>',\n",
       "  'nxtletter=<к',\n",
       "  'nxt2letters=<къ',\n",
       "  'nxt3letters=<къе'],\n",
       " ['bias',\n",
       "  'letterLowercase=к',\n",
       "  'postag=n',\n",
       "  'prevpostag=nprop',\n",
       "  'EOS',\n",
       "  'letterposition=-2',\n",
       "  'prev2letters=ре>',\n",
       "  'prevletter=е>',\n",
       "  'nxtletter=<ъ',\n",
       "  'nxt2letters=<ъе'],\n",
       " ['bias',\n",
       "  'letterLowercase=ъ',\n",
       "  'postag=n',\n",
       "  'prevpostag=nprop',\n",
       "  'EOS',\n",
       "  'letterposition=-1',\n",
       "  'prev3letters=рек>',\n",
       "  'prev2letters=ек>',\n",
       "  'prevletter=к>',\n",
       "  'nxtletter=<е'],\n",
       " ['bias',\n",
       "  'letterLowercase=е',\n",
       "  'postag=n',\n",
       "  'prevpostag=nprop',\n",
       "  'EOS',\n",
       "  'EOW',\n",
       "  'prev4letters=рекъ>',\n",
       "  'prev3letters=екъ>',\n",
       "  'prev2letters=къ>',\n",
       "  'prevletter=ъ>']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, Y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training parameters. L-BFGS (what is this) is default. Using Elastic Net (L1 + L2) regularization [ditto?]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "        'c1': 1.0, #coefficient for L1 penalty\n",
    "        'c2': 1e-3, #coefficient for L2 penalty\n",
    "        'max_iterations': 50 #early stopping\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program saves the trained model to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'LING5800_lezgi.crfsuite'\n",
    "trainer.train(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7efc3091cb38>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use the trained model to make predications for just one example sentence from the test data. The predicted labels are printed out for comparison above the correct labels. Most examples have 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters: в  о  б  щ  е  м  р  а  з  и  х  ь  а  н  а  ч  и  б  у  р  и  м  и  д  и  н  и  ч  и  б  у  б  а  д  и  л  а  г  ь  а  н  а  к  и  в  а  ъ\n",
      "Predicted: B-stem I-stem I-stem I-stem I-stem I-stem B-stem I-stem I-stem I-stem B-stem I-stem I-stem B-AOR I-AOR B-NEG B-stem B-PL I-PL I-PL B-stem I-stem I-stem B-OBL I-OBL B-FOC I-FOC B-stem I-stem B-stem I-stem I-stem I-stem B-ERG I-ERG B-stem I-stem I-stem I-stem I-stem B-AOR I-AOR B-stem I-stem B-stem I-stem I-stem\n",
      "Correct: B-stem I-stem I-stem I-stem I-stem I-stem B-stem I-stem I-stem I-stem B-stem I-stem I-stem B-AOR I-AOR B-NEG B-stem B-PL I-PL I-PL B-stem I-stem I-stem B-OBL I-OBL B-FOC I-FOC B-stem I-stem B-stem I-stem I-stem I-stem B-ERG I-ERG B-stem I-stem I-stem I-stem I-stem B-AOR I-AOR B-stem I-stem B-stem I-stem I-stem\n"
     ]
    }
   ],
   "source": [
    "example_sent = testdata[0]\n",
    "print('Letters:', '  '.join(extractTokens(example_sent)), end='\\n')\n",
    "\n",
    "print('Predicted:', ' '.join(tagger.tag(extractFeatures(example_sent))))\n",
    "print('Correct:', ' '.join(extractLabels(example_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "The following function will evaluate how well the model performs. Unlike CRF example found at https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb, this model is not designed to disregard \"O\" labels, since all characters that are not part of a word (e.g. digits and punctuation) are already eliminated during pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_correct, y_pred):\n",
    "    '''Takes list of correct and predicted labels from tagger.tag. \n",
    "    Prints a classification report for a list of BIO-encoded sequences.\n",
    "    It computes letter-level metrics.'''\n",
    "\n",
    "    labeler = LabelBinarizer()\n",
    "    y_correct_combined = labeler.fit_transform(list(chain.from_iterable(y_correct)))\n",
    "    y_pred_combined = labeler.transform(list(chain.from_iterable(y_pred)))\n",
    "    \n",
    "    tagset = set(labeler.classes_)\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(labeler.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_correct_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will predict BIO labels in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results for labeled position evaluation. This evaluates how well the classifier performed on each morpheme as a whole and their tags, rather than evaluating character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateLabels(y_list):\n",
    "    '''Return list of morpheme labels [[B-label, I-label,...]morph,[B-label,...]]'''\n",
    "    \n",
    "    morphs_list = []\n",
    "    labels_list = []\n",
    "    morph = []\n",
    "    for sent in y_list:\n",
    "        for label in sent:\n",
    "            labels_list.append(label)\n",
    "            if label[0] == 'I':\n",
    "                #build morpheme shape, adding to first letter\n",
    "                morph.append(label)\n",
    "            else:\n",
    "                # Once processed first morph, add new morphemes & gloss labels to output\n",
    "                if morph:\n",
    "                    morphs_list.append(morph)\n",
    "                #Extract morpheme features\n",
    "                morph = [label]\n",
    "    \n",
    "    return morphs_list, labels_list\n",
    "\n",
    "def countMorphemes(morphlist):\n",
    "    counts = {}\n",
    "    for morpheme in morphlist:\n",
    "        counts[morpheme[0][2:]] = counts.get(morpheme[0][2:], 0) + 1\n",
    "    return counts\n",
    "\n",
    "def eval_labeled_positions(y_correct, y_pred):\n",
    "    \n",
    "    #group the labels by morpheme and get list of morphemes\n",
    "    correctmorphs,_ = concatenateLabels(y_correct)\n",
    "    predmorphs,predLabels = concatenateLabels(y_pred)\n",
    "    #Count instances of each morpheme\n",
    "    test_morphcts = countMorphemes(correctmorphs)\n",
    "    pred_morphcts = countMorphemes(predmorphs)\n",
    "    \n",
    "    correctMorphemects = {}\n",
    "    idx = 0\n",
    "    num_correct = 0\n",
    "    for morpheme in correctmorphs:\n",
    "        correct = True\n",
    "        for label in morpheme:\n",
    "            if label != predLabels[idx]:\n",
    "                correct = False\n",
    "            idx += 1\n",
    "        if correct == True:\n",
    "            num_correct += 1\n",
    "            correctMorphemects[morpheme[0][2:]] = correctMorphemects.get(morpheme[0][2:], 0) + 1\n",
    "    #calculate P, R F1 for each morpheme\n",
    "    results = ''\n",
    "    for firstlabel in correctMorphemects.keys():\n",
    "        lprec = correctMorphemects[firstlabel]/pred_morphcts[firstlabel]\n",
    "        lrecall = correctMorphemects[firstlabel]/test_morphcts[firstlabel]\n",
    "        results += firstlabel + '\\t\\t{0:.2f}'.format(lprec) + '\\t\\t' + '{0:.2f}'.format(lrecall) + '\\t' + '{0:.2f}'.format((2*lprec*lrecall)/(lprec+lrecall)) +'\\t\\t' + str(test_morphcts[firstlabel]) + '\\n'\n",
    "    #overall results\n",
    "    precision = num_correct/len(predmorphs)\n",
    "    recall = num_correct/len(correctmorphs)\n",
    "    \n",
    "    print('\\t\\tPrecision\\tRecall\\tf1-score\\tInstances\\n\\n' + results + '\\ntotal/avg\\t{0:.2f}'.format(precision) + '\\t\\t' + '{0:.2f}'.format(recall) + '\\t' + '{0:.2f}'.format((2*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check the results and print a report of the results. These results are for character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tPrecision\tRecall\tf1-score\tInstances\n",
      "\n",
      "stem\t\t0.97\t\t0.97\t0.97\t\t129\n",
      "FOC\t\t0.75\t\t1.00\t0.86\t\t9\n",
      "AOR\t\t1.00\t\t1.00\t1.00\t\t15\n",
      "OBL\t\t0.93\t\t0.59\t0.72\t\t22\n",
      "DAT\t\t1.00\t\t0.88\t0.93\t\t16\n",
      "GEN\t\t0.80\t\t0.57\t0.67\t\t7\n",
      "SUPER\t\t1.00\t\t1.00\t1.00\t\t2\n",
      "ELAT\t\t1.00\t\t1.00\t1.00\t\t3\n",
      "FUT\t\t0.50\t\t1.00\t0.67\t\t1\n",
      "PL\t\t0.50\t\t0.67\t0.57\t\t3\n",
      "PTP\t\t1.00\t\t1.00\t1.00\t\t4\n",
      "TEMP\t\t1.00\t\t1.00\t1.00\t\t1\n",
      "PERF\t\t1.00\t\t1.00\t1.00\t\t1\n",
      "SBST\t\t1.00\t\t1.00\t1.00\t\t1\n",
      "NEG\t\t0.50\t\t1.00\t0.67\t\t1\n",
      "IMPF\t\t1.00\t\t1.00\t1.00\t\t1\n",
      "SUB\t\t1.00\t\t1.00\t1.00\t\t1\n",
      "\n",
      "total/avg\t0.94\t\t0.86\t0.90\n"
     ]
    }
   ],
   "source": [
    "eval_labeled_positions(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-AOR       1.00      1.00      1.00        15\n",
      "       I-AOR       1.00      1.00      1.00        15\n",
      "      B-COND       0.00      0.00      0.00         1\n",
      "      I-COND       0.00      0.00      0.00         2\n",
      "       B-DAT       1.00      1.00      1.00         6\n",
      "      B-ELAT       1.00      1.00      1.00         2\n",
      "      I-ELAT       1.00      1.00      1.00         2\n",
      "       B-ERG       1.00      0.50      0.67         2\n",
      "       I-ERG       1.00      0.50      0.67         2\n",
      "       B-FOC       0.83      1.00      0.91         5\n",
      "       I-FOC       0.83      1.00      0.91         5\n",
      "       B-FUT       1.00      0.50      0.67         2\n",
      "       I-FUT       1.00      0.50      0.67         2\n",
      "       B-GEN       0.75      1.00      0.86         3\n",
      "      B-HORT       0.00      0.00      0.00         1\n",
      "      B-IMPF       1.00      1.00      1.00         3\n",
      "      I-IMPF       1.00      1.00      1.00         7\n",
      "     B-INESS       0.00      0.00      0.00         1\n",
      "       B-NEG       1.00      1.00      1.00         1\n",
      "       B-OBL       0.80      1.00      0.89         8\n",
      "       I-OBL       0.75      1.00      0.86         6\n",
      "        B-PL       1.00      0.60      0.75         5\n",
      "        I-PL       1.00      0.67      0.80         6\n",
      "       B-PTP       1.00      1.00      1.00         2\n",
      "         B-Q       0.00      0.00      0.00         1\n",
      "         I-Q       0.00      0.00      0.00         1\n",
      "       B-SUB       1.00      1.00      1.00         1\n",
      "     B-SUPER       1.00      1.00      1.00         1\n",
      "      B-stem       1.00      1.00      1.00       107\n",
      "      I-stem       0.97      0.98      0.98       322\n",
      "\n",
      "   micro avg       0.97      0.96      0.96       537\n",
      "   macro avg       0.76      0.71      0.72       537\n",
      "weighted avg       0.96      0.96      0.96       537\n",
      " samples avg       0.96      0.96      0.96       537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umoqnier/develop/tesis/workouts/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/umoqnier/develop/tesis/workouts/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, with a 80/20 split, produces an average accuracy of 94% with a less than 2% range over randomized test data. This is significantly above the targeted accuracy of 80%. Table 1 shows the results of one run. \n",
    "\n",
    "|__label__|__precision__|__recall__|__f1-score__|__instances__|\n",
    "|---------|-------------|----------|------------|-------------|\n",
    "|B-AOR|1.00|0.88|0.94|17|\n",
    "|B-DAT|0.92|1.00|0.96|11|\n",
    "|B-ELAT|0.67|1.00|0.80|2|\n",
    "|B-ENT|0.33|0.50|0.40|2|\n",
    "|B-ERG|0.00|0.00|0.00|3|\n",
    "|B-FOC|0.86|1.00|0.92|6|\n",
    "|B-FUT|0.00|0.00|0.00|2|\n",
    "|B-GEN|0.50|0.33|0.40|6|\n",
    "|B-HORT|0.00|0.00|0.00|1|\n",
    "|I|0.95|0.99|0.97|480|\n",
    "|B-INESS|1.00|0.33|0.50|3|\n",
    "|B-MSDR|0.00|0.00|0.00|2|\n",
    "|B-NEG|1.00|1.00|1.00|1|\n",
    "|B-OBL|0.80|0.60|0.69|20|\n",
    "|B-PL|0.50|0.50|0.50|2|\n",
    "|B-POESS|1.00|1.00|1.00|3|\n",
    "|B-PTP|1.00|0.67|0.80|3|\n",
    "|B-SBST|1.00|0.50|0.67|2|\n",
    "|B-SUPER|0.67|1.00|0.80|2|\n",
    "|B-TEMP|0.00|0.00|0.00|1|\n",
    "|B-UNK|0.00|0.00|0.00|1|\n",
    "|B-stem|1.00|0.99|0.99|138|\n",
    "|__avg / total__|__0.94__|__0.94__|__0.94__|__708__|\n",
    "\n",
    "<center>Table 1: Results of morpheme predictions</center>\n",
    "\n",
    "As might be expected, the classifier has less success predicting less frequent labels. This makes the results of the I labels (non-initial letters in a morpheme) surprising, until one considers that transitions between morphemes may not always be clear. Other results become more interesting with some knowledge of Lezgi morphology. The inessive (INESS) and ergative (ERG) case and the oblique stem morpheme (OBL) are identical. The only difference between the first two is the tendency of sentence position, even with Lezgi's free word order. The difference between the latter two is that the ergative morpheme is word final and the the oblique stem is follow by another case morpheme. \n",
    "\n",
    "|__precision__|__recall__|__f1-score__|\n",
    "|-----|------|------|\n",
    "|0.54|0.49|0.49|\n",
    "\n",
    "<center>Table 2: Average score of affix labels only.</center>\n",
    "\n",
    "The classifier has most success identifying stem morphemes (STEM) and non-initial letters (I), the majority of which belong to stem morphemes. It has less success with identifying affixes. The classifier is clearly adept at splitting affixes from stems and this is already helpful to human annotators but it would be less helpful splitting strings of affixes and correcly glossing them. Table 2 shows average precision, recall, and f1-score of affix labels is much less accurate than the overall accuracy. This is most likely due in part to homonymic affixes and in part to the fewer instances of affixes compared to stems. As the more texts are correctly annotated with the help of the model, more data can be fed into the training, hopefully increasing the accuracy and incrementally speeding the annotation process.\n",
    "\n",
    "The data was also run on a bidirectional sequence-to-sequence deep neural network with attention. The hidden layer size was set at 128, the batch size as 32, the teacher forcing ratio at 0.5. The results in Table 3 indicate that with a small amount of data a supervised classifier can produce equal or better results than a neural network.\n",
    "\n",
    "|epochs|accuracy|\n",
    "|------|--------|\n",
    "|50|0.57|\n",
    "|100|0.75|\n",
    "|200|0.90|\n",
    "|300|0.92|\n",
    "|__500__|__0.93__|\n",
    "|600|0.89|\n",
    "|1000|0.91|\n",
    "\n",
    "<center>Table 3: Results of deep neural network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the Classifier Learned\n",
    "\n",
    "By using methods of the crfsuite, we can look insider classifier and see what it learned. From the example printout in Table 3, we can see, for example, that the stem, elative (ELAT), imperfective (IMPF), aorist (AOR), perfective (PF), and plural (PL) morphemes most often consist of more than one letter  but superessive (SUPER), oblique (OBL), and subessive (SUB) morphemes usually consist of just one letter. We can also see that temporal converb (TEMP) morpheme often follows the participle (PTP) morpheme, and another case morpheme tends to follow the oblique, superessive, and subessive case morphemes. These patterns correspond to the facts of Lezgi morphology. On the other hand, both Table 4 and Table 5 indciate that is highly likelythat a  genitive case (GEN) morpheme will be a prefix, which is impossible. This indicates that the affix type (prefix or suffix) might be a useful feature to include.\n",
    "\n",
    "|-|-|-|weights|\n",
    "|---|---|----|-----|\n",
    "|B-SUPER| ->| B-ELAT|  4.820010|\n",
    "|B-OBL|  ->| B-SPSS|  3.806645|\n",
    "|B-SUB|  ->| B-ELAT|  3.444584|\n",
    "|B-OBL|  ->| B-DAT|   2.946830|\n",
    "|B-stem| ->| I|       2.258064|\n",
    "|B-OBL|  ->| B-GEN|   2.247354|\n",
    "|I|      ->| B-OBL|   1.913825|\n",
    "|B-stem| ->| B-OBL|   1.862016|\n",
    "|B-ELAT| ->| I|       1.711584|\n",
    "|B-PTP|  ->| B-TEMP|  1.620690|\n",
    "|B-IMPF| ->| I|       1.300227|\n",
    "|B-AOR|  ->| I|       1.252594|\n",
    "|B-PERF| ->| I|       1.135483|\n",
    "|B-PL|   ->| I|       1.043438|\n",
    "|B-GEN|  ->| B-stem|  0.956780|\n",
    "\n",
    "<center>Table 4: Top most likely transitions</center>\n",
    "\n",
    "On the other hand, Table 5, for example, indicates that the negative affix rarely follows a non-initial letter of another morpheme. This is accurate because the negative affix is the only prefix in the language. It is not surprising that the transition still has a greater than zero probability since it is often only one letter long and this letter may be found at the beginning of any word.\n",
    "\n",
    "|-|-|-|weights|\n",
    "|---|---|---|----|\n",
    "|B-ERG|  ->| B-stem|  0.295926|\n",
    "|B-TEMP| ->| I|       0.254567|\n",
    "|B-SBST| ->| I|       0.249661|\n",
    "|I|      ->| B-NEG|   0.221662|\n",
    "|B-INF|  ->| B-stem|  0.196340|\n",
    "|I|      ->| B-DAT|   0.057729|\n",
    "|B-NEG|  ->| B-stem  |0.013683|\n",
    "|I|      ->| B-stem|  0.009557|\n",
    "|I|      ->| B-ERG|   0.000074|\n",
    "|I|      ->| B-SUPER| -0.000692|\n",
    "|I|      ->| B-FOC|   -0.003919|\n",
    "|I|      ->| B-SBST|  -0.023268|\n",
    "|B-OBL|  ->| I|       -0.034257|\n",
    "|B-INESS| ->| I|       -0.157967|\n",
    "|I|      ->| B-GEN|   -1.180139|\n",
    "\n",
    "<center>Table 5: Top most unlikely transitions</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-stem -> I-stem  7.338007\n",
      "B-ELAT -> I-ELAT  6.792164\n",
      "I-stem -> I-stem  6.099357\n",
      "B-TEMP -> I-TEMP  5.672273\n",
      "B-IMPF -> I-IMPF  5.637289\n",
      "B-INELAT -> I-INELAT 5.626798\n",
      "B-ENT  -> I-ENT   5.488343\n",
      "B-AOR  -> I-AOR   5.305114\n",
      "B-AOC  -> I-AOC   5.273412\n",
      "B-FUT  -> I-FUT   5.196156\n",
      "B-PERF -> I-PERF  5.178553\n",
      "B-OBL  -> I-OBL   5.074970\n",
      "B-PL   -> I-PL    5.025918\n",
      "B-POESS -> I-POESS 5.010790\n",
      "I-PERF -> I-PERF  4.984551\n",
      "\n",
      "Top unlikely transitions:\n",
      "I-ELAT -> B-stem  0.439823\n",
      "I-PL   -> B-ERG   0.379337\n",
      "B-PTP  -> B-TEMP  0.335421\n",
      "I-ERG  -> B-stem  0.322590\n",
      "I-stem -> B-stem  0.293954\n",
      "I-ERG  -> B-FOC   0.290467\n",
      "B-stem -> B-PL    0.199258\n",
      "I-ENT  -> B-NEG   0.194905\n",
      "I-stem -> B-HORT  0.148437\n",
      "I-ENT  -> B-PTP   0.104741\n",
      "B-SPSS -> B-stem  0.074526\n",
      "I-stem -> B-AOC   0.039212\n",
      "I-AOR  -> B-stem  0.002330\n",
      "I-PERF -> B-PTP   0.001912\n",
      "I-stem -> B-GEN   -0.713777\n"
     ]
    }
   ],
   "source": [
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    '''Print info from the crfsuite.'''\n",
    "    \n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some observations about the state features. For example, Table 6 indicates that the model rightly recognized that the stem is nearly always at the beginning of the word and there are no consistent feature to identify the non-initial letters of various morphemes. \n",
    "\n",
    "|weight|label|feature|\n",
    "|---|---|----|\n",
    "|13.385742| B-stem| BOW|\n",
    "|6.80475| I|      bias|\n",
    "|5.169367| B-PL|   nxt2letters=<ур|\n",
    "|5.142534| B-DAT|  letterLowercase=з|\n",
    "|4.858094| B-NEG|  letterLowercase=ш|\n",
    "|4.568794| B-PTP|  letterLowercase=й|\n",
    "|4.513613| B-PST|  letterLowercase=й|\n",
    "|4.361416| B-ADSS| letterLowercase=в|\n",
    "|4.269127| B-PL|   nxtletter=<р|\n",
    "|4.216564| B-FOC|  nxtletter=<и|\n",
    "|4.203677| B-GEN|  letterLowercase=н|\n",
    "|4.023482| B-INF|  letterLowercase=з|\n",
    "|3.977504| B-IMPF| letterLowercase=з|\n",
    "|3.868088| B-NEG|  letterLowercase=ч|\n",
    "|3.636859| B-FOC|  letterLowercase=н|\n",
    "\n",
    "<center>Table 6: Top positive features</center>\n",
    "\n",
    "Table 7 indicates that certain letter sequences might be less likely to begin a morpheme. One interesting observation that could be easily confirmed by a corpus study is that the focus particle is least likely to occur on a verb than on any other lexical category. \n",
    "\n",
    "|weight|label|feature|\n",
    "|---|----|---|\n",
    "|-0.606766| I|      prev2letters=ча>|\n",
    "|-0.679616| I|      letterLowercase=ч|\n",
    "|-0.704380| I|      prevletter=ш>|\n",
    "|-0.741532| I|      prev2letters=ич>|\n",
    "|-0.833423| B-FOC|  postag=v|\n",
    "|-0.937032| B-FOC|  bias|\n",
    "|-1.029693| I|      prev3letters=вал>|\n",
    "|-1.071785| I|      nxtletter=<й|\n",
    "|-1.073034| I|      prev3letters=гьу>|\n",
    "|-1.126576| I|      prev2letters=ди>|\n",
    "|-1.150632| B-AOR|  bias|\n",
    "|-1.201650| I|      letterLowercase=н|\n",
    "|-1.240373| I|      letterLowercase=з|\n",
    "|-1.250568| I|      prevletter=р>|\n",
    "\n",
    "<center>Table 7: Top negative</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "12.498452 B-stem BOW\n",
      "5.503005 B-DAT  letterLowercase=з\n",
      "4.684468 B-GEN  letterLowercase=н\n",
      "4.270364 B-ADSS letterLowercase=в\n",
      "4.201987 B-NEG  letterLowercase=ш\n",
      "4.123131 I-PL   letterLowercase=р\n",
      "3.740163 B-INF  letterLowercase=з\n",
      "3.468910 B-MSDR letterLowercase=н\n",
      "3.321002 B-PL   nxt2letters=<ур\n",
      "3.148994 B-PTP  postag=ptcp\n",
      "3.123837 I-NEG.PST prevpostag=adj\n",
      "3.006395 B-HORT letterLowercase=н\n",
      "2.971788 B-PTP  letterLowercase=й\n",
      "2.937903 B-PST  letterLowercase=й\n",
      "2.905472 I-TEMP prevletter=л>\n",
      "\n",
      "Top negative:\n",
      "-0.808025 B-DAT  postag=v\n",
      "-0.815216 I-stem letterLowercase=в\n",
      "-0.822264 I-stem prev3letters=ава>\n",
      "-0.839039 I-stem prevletter=д>\n",
      "-0.904920 I-stem nxt2letters=<ур\n",
      "-0.996087 I-stem prev2letters=ед>\n",
      "-0.996862 I-stem prev2letters=ча>\n",
      "-1.050426 I-PL   postag=v\n",
      "-1.158361 I-stem prev2letters=уш>\n",
      "-1.293499 I-stem letterLowercase=з\n",
      "-1.349291 I-stem prevletter=н>\n",
      "-1.570701 I-stem letterLowercase=д\n",
      "-1.877090 B-OBL  postag=v\n",
      "-1.973153 I-stem prev2letters=да>\n",
      "-2.218771 I-stem letterLowercase=й\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(15))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future steps\n",
    "\n",
    "The goal of this project was to find a way to speed the work on annotator and improve their accuracy. Since the model reach over the 80% accuracy goal, there seems little reason to try to improve the features, although an examination of the transitions and state features point to a few adjustments that might increase accuracy. The bigggest problem seems to be the almost 50% reduction in predicting the affix glosses. However, the small number of instances found in the test data indicate that this will be improved as the amount of supervised examples increases. The model as it is can speed this increase.\n",
    "\n",
    "It should be assumed that few annotators will have programming skills. This is especially true for speakers of minority languages which are often are in areas with limited educational opportunities. The results of this classifier should be checked and corrected by trained annotators. Ideally, this program would be exapnded to write the predicted breaks and glosses to an XML file compatible with FLEx or ELAN or another interface familiar to the annotator or easy to learn. In meantime, the data could be output to an CSV file and presented to the annotator as an spreadsheet.\n",
    "\n",
    "Even with carefully annotated training data by a linguist familiar with FLEX and Lezgi morphology, mistakes were made. A few POS tags and affix glosses were missing. This prevents the program from working, but does not tell the user where or what the missing data are. Pre-processing functions should be adjusted so that they present the troublesome morphemes with glosses as a list to the user so that they can be found and corrected using FLEx's concordance feature. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tesis-kernel",
   "language": "python",
   "name": "tesis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
