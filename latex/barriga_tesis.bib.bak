% Encoding: UTF-8
% Barriga's references start here

@article{sutton2012introduction,
	title={An introduction to conditional random fields},
	author={Sutton, Charles and McCallum, Andrew and others},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
	volume={4},
	number={4},
	pages={267--373},
	year={2012},
	publisher={Now Publishers, Inc.}
}

@inproceedings{he2004multiscale,
	title={Multiscale conditional random fields for image labeling},
	author={He, Xuming and Zemel, Richard S and Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}},
	booktitle={Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
	volume={2},
	pages={II--II},
	year={2004},
	organization={IEEE}
}

@inproceedings{stern2005modelling,
	title={Modelling uncertainty in the game of Go},
	author={Stern, David H and Graepel, Thore and MacKay, David},
	booktitle={Advances in neural information processing systems},
	pages={1353--1360},
	year={2005}
}

@article{bernal2007global,
	title={Global discriminative learning for higher-accuracy computational gene prediction},
	author={Bernal, Axel and Crammer, Koby and Hatzigeorgiou, Artemis and Pereira, Fernando},
	journal={PLoS computational biology},
	volume={3},
	number={3},
	year={2007},
	publisher={Public Library of Science}
}

@article{lafferty2001conditional,
	title={Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
	author={Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
	year={2001}
}

@article{hammersley1971markov,
	title={Markov fields on finite graphs and lattices},
	author={Hammersley, John M and Clifford, Peter},
	journal={Unpublished manuscript},
	volume={46},
	year={1971}
}

@inproceedings{moeller2018automatic,
	title={Automatic Glossing in a Low-Resource Setting for Language Documentation},
	author={Moeller, Sarah and Hulden, Mans},
	booktitle={Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages},
	pages={84--93},
	year={2018}
}

@Book{lastra1992otomi,
  author    = {Lastra, Yolanda},
  publisher = {Instituto de Investigaciones Antropológicas, UNAM},
  title     = {El otom{\'\i} de Toluca},
  year      = {1992},
  groups    = {Otomi},
}

@InProceedings{lastra2000otomi,
  author    = {Lastra, Yolanda},
  booktitle = {Anales de antropolog{\'\i}a},
  title     = {¿Es el otom{\'\i} una lengua amenazada?},
  year      = {2000},
  volume    = {33},
  groups    = {Otomi},
}

% Mejorar esto

@Book{barrientos2004otomies,
  author    = {Barrientos L{\'o}pez, Guadalupe},
  publisher = {Comisi{\'o}n Nacional para el Desarrollo de los Pueblos Ind{\'i}genas},
  title     = {Otom{\'\i}es del Estado de M{\'e}xico},
  year      = {2004},
  groups    = {Otomi},
}

@book{haspelmath2013understanding,
  title={Understanding morphology},
  author={Haspelmath, Martin and Sims, Andrea D},
  year={2013},
  publisher={Routledge}
}

@misc{mitchell1997machine,
  title={Machine learning},
  author={Mitchell, Tom M and others},
  year={1997},
  publisher={McGraw-hill New York}
}

@misc{CRFsuite,
    author = {Naoaki Okazaki},
    title = {CRFsuite: a fast implementation of Conditional Random Fields (CRFs)},
    urls= {http://www.chokkan.org/software/crfsuite/},
    year = {2007}
}

@article{jurafsky2008speech,
  title={Speech and Language Processing: An introduction to speech recognition, computational linguistics and natural language processing},
  author={Jurafsky, Daniel and Martin, James H},
  journal={Upper Saddle River, NJ: Prentice Hall},
  year={2008}
}

@article{comrie2008leipzig,
  title={The Leipzig Glossing Rules: Conventions for interlinear morpheme-by-morpheme glosses},
  author={Comrie, Bernard and Haspelmath, Martin and Bickel, Balthasar},
  journal={Department of Linguistics of the Max Planck Institute for Evolutionary Anthropology \& the Department of Linguistics of the University of Leipzig. Retrieved January},
  volume={28},
  pages={2010},
  year={2008}
}

@Misc{elotl2019otomiprepro,
  author = {Comunidad Elotl},
  note   = {Acceso: 2020-05-08},
  title  = {Procesando el otomí (hñähñu) ¿Dónde empezar?},
  year   = {2019},
  groups = {Otomi},
  urls   = {https://elotl.mx/2019/01/procesando-el-otomi-hnahnu-donde-empezar/},
}

@article{vasquesextraccion,
  author={Vasques, Mar{\'i}a Ximena Guti{\'e}rrez},
  title={EXTRACCI{\'O}N L{\'E}XICA BILING{\"U}E AUTOM{\'A}TICA PARA LENGUAS DE BAJOS RECURSOS DIGITALES},
  year = {2018}
}


@misc{anastasopoulos2018partofspeech,
    title={Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource},
    author={Antonis Anastasopoulos and Marika Lekakou and Josep Quer and Eleni Zimianiti and Justin DeBenedetto and David Chiang},
    year={2018},
    eprint={1806.03757},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@Article{Brown2020,
  author      = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal     = {ArXiv preprint arXiv:2005.14165},
  title       = {Language Models are Few-Shot Learners},
  year        = {2020},
  abstract    = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  date        = {2020-05-28},
  eprint      = {2005.14165},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/2005.14165v4:PDF},
  keywords    = {cs.CL},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Otomi\;0\;0\;0x4d8080ff\;\;Cites related to the otomi languague\;;
}
